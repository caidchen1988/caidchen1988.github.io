{"posts":[{"title":"面试二：Java SPI机制原理和应用","content":"前言 首先我们了解下什么是SPI？在具体那些场景中有应用？和Dubbo中的SPI有什么区别？ 一、什么是SPI？ SPI 的全称为(Service Provider Interface)，是JDK内置的一种服务提供发现机制。 可以用来启用框架扩展和替换组件，主要是被框架的开发人员使用，比如java.sql.Driver接口，不同厂商可以针对同一接口做出不同的实现，MySQL、PostgreSQL和Oracle都有不同的实现提供给用户，而Java的SPI机制可以为某个接口寻找服务实现。 Java中SPI机制主要思想是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要，其核心思想就是解耦。 二、在具体那些场景中有应用？ 那么如何才能实现Java SPI呢，分如下几步： 提供接口实现 在classpath下的META-INF/services/目录里创建一个以服务接口命名的文件（全路径） 文件里面记录的是此 jar 包提供的具体实现类的全限定名 让我们看下Mysql如何实现的 示例 定义接口 public interface Driver { void getConnect(); } Mysql实现 public class MysqlDriver implements Driver { @Override public void getConnect() { System.out.println(&quot;Mysql驱动连接&quot;); } } PostgreSQL实现 public class PostgreSQL implements Driver { @Override public void getConnect() { System.out.println(&quot;PostgreSQL驱动连接&quot;); } } 启动类 public class SPIDemo { public static void main(String[] args) { ServiceLoader&lt;Driver&gt; s = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; iterator = s.iterator(); while (iterator.hasNext()) { Driver driver = iterator.next(); driver.getConnect(); } } } META-INF/services配置 com.example.spi.MysqlDriver com.example.spi.PostgreSQL 查看执行结果： 三、SPI机制的广泛应用 1、SPI机制 - JDBC DriverManager 在JDBC4.0之前，我们开发有连接数据库的时候，通常会用Class.forName(&quot;com.mysql.jdbc.Driver&quot;)这句先加载数据库相关的驱动，然后再进行获取连接等的操作。 而JDBC4.0之后不需要用Class.forName(&quot;com.mysql.jdbc.Driver&quot;)来加载驱动，直接获取连接就可以了，现在这种方式就是使用了Java的SPI扩展机制来实现。 JDBC接口定义 首先在java中定义了接口java.sql.Driver，并没有具体的实现，具体的实现都是由不同厂商来提供的。 mysql实现 在mysql的jar包mysql-connector-java-6.0.6.jar中，可以找到META-INF/services目录，该目录下会有一个名字为java.sql.Driver的文件，文件内容是com.mysql.cj.jdbc.Driver，这里面的内容就是针对Java中定义的接口的实现。 postgresql实现 同样在postgresql的jar包postgresql-42.0.0.jar中，也可以找到同样的配置文件，文件内容是org.postgresql.Driver，这是postgresql对Java的java.sql.Driver的实现。 使用方法 上面说了，现在使用SPI扩展来加载具体的驱动，我们在Java中写连接数据库的代码的时候，不需要再使用Class.forName(&quot;com.mysql.jdbc.Driver&quot;)来加载驱动了，而是直接使用如下代码： String url = &quot;jdbc:xxxx://xxxx:xxxx/xxxx&quot;; Connection conn = DriverManager.getConnection(url,username,password); 源码实现 上面的代码可以直接获取数据库连接进行操作，但是跟SPI有啥关系呢？上面代码没有了加载驱动的代码，我们怎么去确定使用哪个数据库连接的驱动呢？这里就涉及到使用Java的SPI扩展机制来查找相关驱动的东西了，关于驱动的查找其实都在DriverManager中，DriverManager是Java中的实现，用来获取数据库连接，在DriverManager中有一个静态代码块如下： static { loadInitialDrivers(); println(&quot;JDBC DriverManager initialized&quot;); } 接着看loadInitialDrivers方法： 上面的代码主要步骤是： 从系统变量中获取有关驱动的定义。 使用SPI来获取驱动的实现。 遍历使用SPI获取到的具体实现，实例化各个实现类。 根据第一步获取到的驱动列表来实例化具体实现类。 2、SPI机制 - Common-Logging common-logging（也称Jakarta Commons Logging，缩写 JCL）是常用的日志库门面。我们看下它是怎么解耦的。 接口定义 日志实例是通过LogFactory的getLog(String)方法创建的： LogFatory实现 LogFatory是一个抽象类，它负责加载具体的日志实现，分析其Factory getFactory()方法： 可以看出，抽象类LogFactory加载具体实现的步骤如下： 从vm系统属性org.apache.commons.logging.LogFactory 使用SPI服务发现机制，发现org.apache.commons.logging.LogFactory的实现 查找classpath根目录commons-logging.properties的org.apache.commons.logging.LogFactory属性是否指定factory实现 使用默认factory实现，org.apache.commons.logging.impl.LogFactoryImpl 3、SPI机制 - 插件体系 其实最具spi思想的应该属于插件开发，我们项目中也用到的这种思想，具体说一下eclipse的插件思想。 Eclipse使用OSGi作为插件系统的基础，动态添加新插件和停止现有插件，以动态的方式管理组件生命周期。 一般来说，插件的文件结构必须在指定目录下包含以下三个文件： META-INF/MANIFEST.MF: 项目基本配置信息，版本、名称、启动器等 build.properties: 项目的编译配置信息，包括，源代码路径、输出路径 plugin.xml：插件的操作配置信息，包含弹出菜单及点击菜单后对应的操作执行类等 当eclipse启动时，会遍历plugins文件夹中的目录，扫描每个插件的清单文件MANIFEST.MF，并建立一个内部模型来记录它所找到的每个插件的信息，就实现了动态添加新的插件。 这也意味着是eclipse制定了一系列的规则，像是文件结构、类型、参数等。插件开发者遵循这些规则去开发自己的插件，eclipse并不需要知道插件具体是怎样开发的，只需要在启动的时候根据配置文件解析、加载到系统里就好了，是spi思想的一种体现。 4、SPI机制 - Spring中SPI机制 在springboot的自动装配过程中，最终会加载META-INF/spring.factories文件，而加载的过程是由SpringFactoriesLoader加载的。 从CLASSPATH下的每个Jar包中搜寻所有META-INF/spring.factories配置文件，然后将解析properties文件，找到指定名称的配置后返回。 需要注意的是，其实这里不仅仅是会去ClassPath路径下查找，会扫描所有路径下的Jar包，只不过这个文件只会在Classpath下的jar包中。 四、SPI和API的区别 SPI - “接口”位于“调用方”所在的“包”中 概念上更依赖调用方。 组织上位于调用方所在的包中。 实现位于独立的包中。 常见的例子是：插件模式的插件。 API - “接口”位于“实现方”所在的“包”中 概念上更接近实现方。 组织上位于实现方所在的包中。 实现和接口在一个包中。 常见的例子是：开放平台接口 五、SPI机制的缺陷 通过上面的解析，可以发现，我们使用SPI机制的缺陷： 不能按需加载，需要遍历所有的实现，并实例化，然后在循环中才能找到我们需要的实现。如果不想用某些实现类，或者某些类实例化很耗时，它也被载入并实例化了，这就造成了浪费。 获取某个实现类的方式不够灵活，只能通过 Iterator 形式获取，不能根据某个参数来获取对应的实现类。 多个并发多线程使用 ServiceLoader 类的实例是不安全的。 六、和Dubbo SPI区别？ 因为SPI机制中的有如上之缺陷，因此 Dubbo 就自己实现了一个 SPI，除了解决按需加载问题，增加了 IOC 和 AOP 的特性，还有个自适应扩展机制。 我们先来看一下 Dubbo 对配置文件目录的约定，不同于 Java SPI ，Dubbo 分为了三类目录。 META-INF/services/ 目录：该目录下的 SPI 配置文件是为了用来兼容 Java SPI 。 META-INF/dubbo/ 目录：该目录存放用户自定义的 SPI 配置文件。 META-INF/dubbo/internal/ 目录：该目录存放 Dubbo 内部使用的 SPI 配置文件。 Dubbo SPI 简单实例 首先在 META-INF/dubbo 目录下按接口全限定名建立一个文件，内容如下： dog=com.sunnick.animal.impl.Dog cat=com.sunnick.animal.impl.Cat 然后在接口上标注@SPI注解，以表明它要用SPI机制 @SPI public interface Animal { void run(); } 接着通过下面的示例代码即可加载指定的实现类。 public class Cat implements Animal{ @Override public void run() { System.out.println(&quot;小猫步走起来～&quot;); } } public class Dog implements Animal { @Override public void run() { System.out.println(&quot;小狗飞奔～&quot;); } } 测试方法： public void testDubboSPI(){ System.out.println(&quot;======dubbo SPI======&quot;); ExtensionLoader&lt;Animal&gt; extensionLoader = ExtensionLoader.getExtensionLoader(Animal.class); Animal cat = extensionLoader.getExtension(&quot;cat&quot;); cat.run(); Animal dog = extensionLoader.getExtension(&quot;dog&quot;); dog.run(); } 测试结果如下： ======dubbo SPI====== 小猫步走起来～ 小狗飞奔～ ","link":"https://caidchen1988.github.io/post/mian-shi-er-java-spi-ji-zhi-yuan-li-he-ying-yong/"},{"title":"面试一：StringBuffer 和 StringBuilder 区别","content":"面试官：StringBuilder和StringBuffer的区别在哪？ 我：StringBuilder不是线程安全的，StringBuffer是线程安全的 面试官：那StringBuilder不安全的点在哪儿？ 我：。。。 StringBuffer 之间的最大不同在于 StringBuilder 的方法不是线程安全的。 由于 StringBuilder 相较于 StringBuffer 有速度优势，所以多数情况下建议使用 StringBuilder 类。 分析 public class StringBuilderDemo { public static void main(String[] args) throws InterruptedException { StringBuilder stringBuilder = new StringBuilder(); for (int i = 0; i &lt; 10; i++){ new Thread(() -&gt; { for (int j = 0; j &lt; 1000; j++){ stringBuilder.append(&quot;a&quot;); } }).start(); } Thread.sleep(100); System.out.println(stringBuilder.length()); } } 我们能看到这段代码创建了10个线程，每个线程循环1000次往StringBuilder对象里面append字符。正常情况下代码应该输出10000，但是实际运行会输出什么呢？ 我们看到输出了“9173”，小于预期的10000，并且还抛出了一个ArrayIndexOutOfBoundsException异常（异常不是必现）。 1、为什么输出值跟预期值不一样 我们先看一下StringBuilder.append()方法的实现，该方法调用父类AbstractStringBuilder的append()方法。 先不管其他代码，count += len;就不是一个原子操作，多线程场景就会出现问题。 2、为什么会抛出ArrayIndexOutOfBoundsException异常。 我们看回AbstractStringBuilder的append()方法源码的ensureCapacityInternal()方法是检查StringBuilder对象的原char数组的容量能不能盛下新的字符串，如果盛不下就调用expandCapacity()方法对char数组进行扩容。 扩容的逻辑就是new一个新的char数组，新的char数组的容量是原来char数组的两倍再加2，再通过System.arryCopy()函数将原数组的内容复制到新数组，最后将指针指向新的char数组，问题就出在此处。 假设现在有两个线程同时执行了StringBuilder的append()方法，两个线程都执行完了ensureCapacityInternal()方法，此刻count=5。 这个时候线程1的cpu时间片用完了，线程2继续执行。线程2执行完整个append()方法后count变成6了 线程1继续执行的时候拿到的count值就是6了，执行char数组拷贝的时候就会抛出ArrayIndexOutOfBoundsException异常。 3、StringBuffer用什么手段保证线程安全的？ 打开append()源码发现其实就是通过同步锁synchronized保证了线程安全。 ","link":"https://caidchen1988.github.io/post/mian-shi-yi-stringbuffer-he-stringbuilder-qu-bie/"},{"title":"Java源码分析之线程池","content":"首先所有的需求都是机遇业务来分析，先抛问题： 如果当前线程池的活跃线程是 3 个（2 个核心线程+ 1 个非核心线程），但是它们各自的任务都执行完成了。然后我每隔 3 秒往线程池里面扔一个耗时 1 秒的任务。那么 30 秒之后，活跃线程数是多少？ 答案：3个 DEMO代码 public class ThreadTest { @Test public void test() throws InterruptedException { ThreadPoolExecutor executorService = new ThreadPoolExecutor(2, 3, 30, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(2), new DefaultThreadFactory(&quot;test&quot;), new ThreadPoolExecutor.DiscardPolicy()); //每隔两秒打印线程池的信息 ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); scheduledExecutorService.scheduleAtFixedRate(() -&gt; { System.out.println(&quot;=====================================thread-pool-info:&quot; + new Date() + &quot;=====================================&quot;); System.out.println(&quot;CorePoolSize:&quot; + executorService.getCorePoolSize()); System.out.println(&quot;PoolSize:&quot; + executorService.getPoolSize()); System.out.println(&quot;ActiveCount:&quot; + executorService.getActiveCount()); System.out.println(&quot;KeepAliveTime:&quot; + executorService.getKeepAliveTime(TimeUnit.SECONDS)); System.out.println(&quot;QueueSize:&quot; + executorService.getQueue().size()); }, 0, 2, TimeUnit.SECONDS); try { //同时提交5个任务,模拟达到最大线程数 for (int i = 0; i &lt; 5; i++) { executorService.execute(new Task()); } } catch (Exception e) { e.printStackTrace(); } //休眠10秒，打印日志，观察线程池状态 Thread.sleep(10000); //每隔3秒提交一个任务 while (true) { Thread.sleep(3000); executorService.submit(new Task()); } } static class Task implements Runnable { @Override public void run(){ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + &quot;-执行任务&quot;); } } } 这个程序的运行结果是这样的： 一共五个任务，线程池的运行情况是什么样的呢？ 先看标号为 1 的地方： 三个线程都在执行任务，然后 2 号线程和 1 号线程率先完成了任务，接着把队列里面的两个任务拿出来执行（标号为 2 的地方）。 按照程序，接下来，每隔 3 秒就有一个耗时 1 秒的任务过来。而此时线程池里面的三个活跃线程都是空闲状态。 那么问题就来了： 该选择哪个线程来执行这个任务呢？是随机选一个吗？ 即在我们的案例中，虽然线程都是空闲的，但是当任务来的时候不是随机调用的，而是轮询。 由于是轮询，每三秒执行一次，所以非核心线程的空闲时间最多也就是 9 秒，不会超过 30 秒，所以一直不会被回收。 为什么是轮询？ 我们通过 Demo 验证了在上面场景中线程执行顺序为轮询，那么为什么呢？ 先Dump下线程，日志如下： 接着根据堆栈信息，我们可以定位到这里的源码： java.util.concurrent.locks.AbstractQueuedSynchronizer.ConditionObject#awaitNanos 通过源码我们可以理解成生产者-消费者的问题，三个线程就是三个消费者，现在没有任务需要处理，它们就等着生产者生产任务，然后通知它们准备消费。 可以看到 addConditionWaiter 方法其实就是在操作我们要找的那个队列，学名叫做等待队列。 Debug 一下，看看队列里面的情况： 巧了嘛，这不是。顺序刚好是： Thread[test-1-3,5,main] Thread[test-1-2,5,main] Thread[test-1-1,5,main] 消费者这边我们大概摸清楚了，接着去看看生产者。 java.util.concurrent.ThreadPoolExecutor#execute 线程池是在这里把任务放到队列里面去的。 而这个方法里面的源码是这样的： 其中signalNotEmpty() 最终会走到 doSignal 方法，而该方法里面会调用 transferForSignal 方法。 这个方法里面会调用 LockSupport.unpark(node.thred)方法，唤醒线程： 而唤醒的顺序，就是等待队列里面的顺序： 所以，现在你知道当一个任务来了之后，这个任务该由线程池里面的哪个线程执行，这个不是随机的，也不是随便来的，是讲究一个顺序的。 什么顺序呢？ Condition 里面的等待队列里面的顺序。详细了解请参考：https://blog.csdn.net/bohu83/article/details/51098106 非核心线程怎么回收？ 还是上面的例子，假设非核心线程就空闲了超过 30 秒，那么它是怎么被回收的呢？ 这个也是一个比较热门的面试题。 java.util.concurrent.ThreadPoolExecutor#getTask 当 timed 参数为 true 的时候，会执行 workQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)方法。 而 timed 什么时候为 true 呢？ boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; 参数解说： allowCoreThreadTimeOut为true 该值为true，则线程池数量最后销毁到0个。 allowCoreThreadTimeOut为false 销毁机制：超过核心线程数时，而且（超过最大值或者timeout过），就会销毁。 allowCoreThreadTimeOut 默认为 false。 所以，就是看wc &gt; corePoolSize条件，wc 是活跃线程数。此时活跃线程数为 3 ，大于核心线程数 2。 最终执行ThreadPoolExecutor#processWorkerExit.remove()回收线程。 ","link":"https://caidchen1988.github.io/post/java-yuan-ma-fen-xi-zhi-xian-cheng-chi/"},{"title":"RocketMQ消息幂等（去重）通用解决方案","content":"问题出现场景 消息中间件是一个可靠的组件，只要我把消息成功投递到了消息中间件，消息就不会丢失，即消息肯定会至少保证消息能被消费者成功消费一次，也就是我们常说的“AT LEAST ONCE”，即消息至少会被“成功消费一遍”。 举个例子，一个消息M发送到了消息中间件，消息投递到了消费程序A，A接受到了消息，然后进行消费，但在消费到一半的时候程序重启了，这时候这个消息并没有标记为消费成功，这个消息还会继续投递给这个消费者，直到其消费成功了，消息中间件才会停止投递。 然而这种可靠的特性导致，消息可能被多次地投递。举个例子，还是刚刚这个例子，程序A接受到这个消息M并完成消费逻辑之后，正想通知消息中间件“我已经消费成功了”的时候，程序就重启了，那么对于消息中间件来说，这个消息并没有成功消费过，所以他还会继续投递。这时候对于应用程序A来说，看起来就是这个消息明明消费成功了，但是消息中间件还在重复投递。 基于消息的投递可靠（消息不丢）是优先级更高的，所以消息不重的任务就会转移到应用程序自我实现，这也是为什么RocketMQ的文档里强调的，消费逻辑需要自我实现幂等。背后的逻辑其实就是：不丢和不重是矛盾的（在分布式场景下），但消息重复是有解决方案的，而消息丢失是很麻烦的。 解决方案 简单的消息去重解决方案 例如：假设我们业务的消息消费逻辑是：插入某张订单表的数据，然后更新库存： insert into t_order values ..... update t_inv set count = count-1 where good_id = 'good123'; 要实现消息的幂等，我们可能会采取这样的方案： select * from t_order where order_no = 'order123' if(order != null) { return ; //消息重复，直接返回 } 这对于很多情况下，的确能起到不错的效果，但是在并发场景下，还是会有问题。 并发重复消息 假设这个消费的所有代码加起来需要1秒，有重复的消息在这1秒内（假设100毫秒）内到达（例如生产者快速重发，Broker重启等），那么很可能，上面去重代码里面会发现，数据依然是空的（因为上一条消息还没消费完，还没成功更新订单状态），那么就会穿透掉检查的挡板，最后导致重复的消息消费逻辑进入到非幂等安全的业务代码中，从而引发重复消费的问题（如主键冲突抛出异常、库存被重复扣减而没释放等） 要解决上面并发场景下的消息幂等问题，一个可取的方案是开启事务把select 改成 select for update语句，把记录进行锁定。 select * from t_order where order_no = 'THIS_ORDER_NO' for update //开启事务 if(order.status != null) { return ;//消息重复，直接返回 } 问题：但这样消费的逻辑会因为引入了事务包裹而导致整个消息消费可能变长，并发度下降。 基于关系数据库事务插入消息表 要实现消息只被消费一次（并且肯定要保证能消费一次），我们可以这样做：在这个数据库中增加一个消息消费记录表，把消息插入到这个表，并且把原来的订单更新和这个插入的动作放到同一个事务中一起提交，就能保证消息只会被消费一遍了。 开启事务 插入消息表（处理好主键冲突的问题） 更新订单表（原消费逻辑） 提交事务 说明： 1、这时候如果消息消费成功并且事务提交了，那么消息表就插入成功了，这时候就算RocketMQ还没有收到消费位点的更新再次投递，也会插入消息失败而视为已经消费过，后续就直接更新消费位点了。这保证我们消费代码只会执行一次。 2、如果事务提交之前服务挂了（例如重启），对于本地事务并没有执行所以订单没有更新，消息表也没插入成功；而对于RocketMQ服务端来说，消费位点也没更新，所以消息还会继续投递下来，投递下来发现这个消息插入消息表也是成功的，所以可以继续消费。这保证了消息不丢失。 局限性： 1、消息的消费逻辑必须是依赖于关系型数据库事务。如果消费的消费过程中还涉及其他数据的修改，例如Redis这种不支持事务特性的数据源，则这些数据是不可回滚的。 2、数据库的数据必须是在一个库，跨库无法解决 注：业务上，消息表的设计不应该以消息ID作为标识，而应该以业务的业务主键作为标识更为合理，以应对生产者的重发。阿里云上的消息去重只是RocketMQ的messageId，在生产者因为某些原因手动重发（例如上游针对一个交易重复请求了）的场景下起不到去重/幂等的效果（因消息id不同）。 更复杂的业务场景 如上所述，这种方式的实现，实际上有很多局限性，这种局限性使得这个方案基本不具备广泛应用的价值。并且由于基于事务，可能导致锁表时间过长等性能问题。 例如我们以一个比较常见的一个订单申请的消息来举例，可能有以下几步（以下统称为步骤X）： 检查库存（RPC） 锁库存（RPC） 开启事务，插入订单表（MySQL） 调用某些其他下游服务（RPC） 更新订单状态 commit 事务（MySQL） 这种情况下，我们如果采取消息表+本地事务的实现方式，消息消费过程中很多子过程是不支持回滚的，也就是说就算我们加了事务，实际上这背后的操作并不是原子性的。怎么说呢，就是说有可能第一条小在经历了第二步锁库存的时候，服务重启了，这时候实际上库存是已经在另外的服务里被锁定了，这并不能被回滚。当然消息还会再次投递下来，要保证消息能至少消费一遍，换句话说，锁库存的这个RPC接口本身依旧要支持“幂等”。 再者，如果在这个比较耗时的长链条场景下加入事务的包裹，将大大的降低系统的并发。所以通常情况下，我们处理这种场景的消息去重的方法还是会使用一开始说的业务自己实现去重逻辑的方式，如前面加select for update，或者使用乐观锁。 拆解消息执行过程 其中一个思路是把上面的几步，拆解成几个不同的子消息，例如： 库存系统消费A：检查库存并做锁库存，发送消息B给订单服务 订单系统消费消息B：插入订单表（MySQL），发送消息C给自己（下游系统）消费 下游系统消费消息C：处理部分逻辑，发送消息D给订单系统 订单系统消费消息D：更新订单状态 注：上述步骤需要保证本地事务和消息是一个事务的（至少是最终一致性的），这其中涉及到分布式事务消息相关的话题，不在本文论述。 可以看到这样的处理方法会使得每一步的操作都比较原子，而原子则意味着是小事务，小事务则意味着使用消息表+事务的方案显得可行。 然而，这太复杂了！这把一个本来连续的代码逻辑割裂成多个系统多次消息交互！那还不如业务代码层面上加锁实现呢。 更通用的解决方案 上面消息表+本地事务的方案之所以有其局限性和并发的短板，究其根本是因为它依赖于关系型数据库的事务，且必须要把事务包裹于整个消息消费的环节。 如果我们能不依赖事务而实现消息的去重，那么方案就能推广到更复杂的场景例如：RPC、跨库等。 例如，我们依旧使用消息表，但是不依赖事务，而是针对消息表增加消费状态，是否可以解决问题呢？ 以上是去事务化后的消息幂等方案的流程，可以看到，此方案是无事务的，而是针对消息表本身做了状态的区分：消费中、消费完成。只有消费完成的消息才会被幂等处理掉。而对于已有消费中的消息，后面重复的消息会触发延迟消费（在RocketMQ的场景下即发送到RETRY TOPIC），之所以触发延迟消费是为了控制并发场景下，第二条消息在第一条消息没完成的过程中，去控制消息不丢（如果直接幂等，那么会丢失消息（同一个消息id的话），因为上一条消息如果没有消费完成的时候，第二条消息你已经告诉broker成功了，那么第一条消息这时候失败broker也不会重新投递了） 我们一开始想解决的问题是否解决了： 消息已经消费成功了，第二条消息将被直接幂等处理掉（消费成功）。 并发场景下的消息，依旧能满足不会出现消息重复，即穿透幂等挡板的问题。 支持上游业务生产者重发的业务重复的消息幂等问题。 关于第一个问题已经很明显已经解决了，在此就不讨论了。 关于第二个问题是如何解决的？主要是依靠插入消息表的这个动作做控制的，假设我们用MySQL作为消息表的存储媒介（设置消息的唯一ID为主键），那么插入的动作只有一条消息会成功，后面的消息插入会由于主键冲突而失败，走向延迟消费的分支，然后后面延迟消费的时候就会变成上面第一个场景的问题。 关于第三个问题，只要我们设计去重的消息键让其支持业务的主键（例如订单号、请求流水号等），而不仅仅是messageId即可。所以也不是问题。 此方案是否有消息丢失的风险？ 会发现这里实际上是有逻辑漏洞的，问题出在上面聊到的个三问题中的第2个问题（并发场景），在并发场景下我们依赖于消息状态是做并发控制使得第2条消息重复的消息会不断延迟消费（重试）。但如果这时候第1条消息也由于一些异常原因（例如机器重启了、外部异常导致消费失败）没有成功消费成功呢？也就是说这时候延迟消费实际上每次下来看到的都是消费中的状态，最后消费就会被视为消费失败而被投递到死信Topic中（RocketMQ默认可以重复消费16次）。 有这种顾虑是正确的！对于此，我们解决的方法是，插入的消息表必须要带一个最长消费过期时间，例如10分钟，意思是如果一个消息处于消费中超过10分钟，就需要从消息表中删除（需要程序自行实现）。所以最后这个消息的流程会是这样的： 更灵活的消息表存储媒介 我们这个方案实际上没有事务的，只需要一个存储的中心媒介，那么自然我们可以选择更灵活的存储媒介，例如Redis。使用Redis有两个好处： 性能上损耗更低 上面我们讲到的超时时间可以直接利用Redis本身的ttl实现 当然Redis存储的数据可靠性、一致性等方面是不如MySQL的，需要用户自己取舍。 ","link":"https://caidchen1988.github.io/post/rocketmq-xiao-xi-mi-deng-qu-chong-tong-yong-jie-jue-fang-an/"},{"title":"Hello Gridea","content":"👏 欢迎使用 Gridea ！ ✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... Github Gridea 主页 示例网站 特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作 🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片 🏷️ 你可以对文章进行标签分组 📋 你可以自定义菜单，甚至可以创建外部链接菜单 💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端 🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台 💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统 🇬🇧 你可以使用中文简体或英语 🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力 🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步 🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃 未来，它一定会成为你离不开的伙伴 尽情发挥你的才华吧！ 😘 Enjoy~ ","link":"https://caidchen1988.github.io/post/hello-gridea/"}]}